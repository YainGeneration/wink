# spotify main pipeline

"""
Agent3 (í†µí•© íŒŒì´í”„ë¼ì¸)
- Agent 1: í•œêµ­ì–´ â†’ ì˜ì–´ (EXAONE)
- Agent 2: ì´ë¯¸ì§€ â†’ ì˜ì–´ ìº¡ì…˜ (Gemma3)
- Agent 3: ë¬¸ì¥ í•©ì„± + í‚¤ì›Œë“œ ì¶”ì¶œ (Gemma3)
- ì¶”ì²œ: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ Spotify ì˜¤ë””ì˜¤í”¼ì²˜ ì¶”ì²œ
- ì„¸ì…˜ ì €ì¥: active_session.json
"""

import os
import re
import json
from datetime import datetime
import requests
import uuid

# --------------------------
# Agent 1: ë²ˆì—­ ëª¨ë“ˆ
# --------------------------
try:
    from agent1_exaone import korean_to_english
except ImportError:
    print("âŒ 'agent1_exaone.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --------------------------
# Agent 2: ì´ë¯¸ì§€ ìº¡ì…”ë‹
# --------------------------
try:
    from agent2_imageToEng import image_to_english_caption
except ImportError:
    print("âŒ 'agent2_imageToEng.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --------------------------
# Context Manager
# --------------------------
try:
    from context_manager import get_full_conversation_history
except ImportError:
    print("âŒ 'context_manager.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --------------------------
# Cosine Recommender
# --------------------------
try:
    from cosine_similarity_recommend import recommend, get_album_cover_url, get_preview_url
except ImportError:
    print("âŒ 'spotify/cosine_similarity_recommend.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# =========================================================
# ì „ì—­ ì„¤ì •
# =========================================================
OLLAMA_URL = "http://localhost:11434"
GEMMA3_MODEL = "gemma3:27b"
SAVE_DIR = "agents/keywords"
os.makedirs(SAVE_DIR, exist_ok=True)


# =========================================================
# Agent 3-1: ë¬¸ì¥ ë³‘í•© (Creation X -> Combination O)
# =========================================================
def rewrite_combined_sentence(text1: str, text2: str, full_history: str) -> str:    
    # ì…ë ¥ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    if not text1 and not text2:
        print("âš ï¸ [Agent 3] No input text provided.")
        return ""

    print("ğŸ§© [Agent 3] Merging sentences (Strict Combination)...")

    # [í•µì‹¬ ë³€ê²½] 'ì‘ê°€'ê°€ ì•„ë‹ˆë¼ 'í¸ì§‘ì' ëª¨ë“œë¡œ ë³€ê²½
    # ë‚´ìš©ì„ ìƒˆë¡œ ì“°ì§€ ë§ê³ , ë‘ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‡ê¸°ë§Œ í•˜ë¼ê³  ì§€ì‹œ
    prompt = f"""
You are a text editor.
Your task is to **logically connect** the User Text and the Visual Context into one clear English sentence.

[Inputs]
1. **User Request (Primary)**: "{text1}"
2. **Visual Context (Secondary)**: "{text2}"
3. **Context History**: "{full_history}"

[Strict Rules]
- **DO NOT invent new emotions or adjectives.**
- **DO NOT change the User Request.** Keep the specific nouns (e.g., Rain, Winter, Drive) exactly as they are.
- Structure: "[User Request], while [Visual Context description]." or similar logical connection.
- Keep it concise and factual.
"""

    messages = [{"role": "user", "content": prompt}]
    
    payload = {
        "model": GEMMA3_MODEL, 
        "messages": messages, 
        "stream": False, 
        "options": {
            "temperature": 0.2,  # ì°½ì˜ì„±ì„ í™• ë‚®ì¶°ì„œ(0.2) ìˆëŠ” ê·¸ëŒ€ë¡œ í•©ì¹˜ê²Œ ìœ ë„
            "num_ctx": 4096
        }
    }

    try:
        res = requests.post(f"{OLLAMA_URL}/api/chat", json=payload, timeout=120)
        res.raise_for_status()
        
        raw = res.json().get("message", {}).get("content", "").strip()
        print(f"ğŸ” [Debug] Raw LLM Output: {raw}") 

        # ë”°ì˜´í‘œ ì œê±°
        cleaned_sentence = raw.strip().strip('"').strip("'")
        
        if len(cleaned_sentence) < 5:
             raise ValueError("Output too short")

        return cleaned_sentence

    except Exception as e:
        print(f"âš ï¸ Merge failed: {e}")
        print("ğŸ‘‰ Using simple concatenation as fallback.")
        
        # LLM ì‹¤íŒ¨ ì‹œ, ê·¸ëƒ¥ ë¬¼ë¦¬ì ìœ¼ë¡œ í•©ì³ì„œ ë°˜í™˜ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
        if text1 and text2:
            return f"{text1}. Context: {text2}"
        return text1 if text1 else text2
    
# =========================================================
# Agent 3-2: í‚¤ì›Œë“œ ì¶”ì¶œ (ì†Œë¦¬ + í™˜ê²½/ë°°ê²½ ë°¸ëŸ°ìŠ¤)
# =========================================================
def extract_keywords(merged_text: str, k: int = 5):
    if not merged_text.strip():
        return []

    print("ğŸ’¬ [Agent 3] Extracting vibe & atmospheric keywords...")

    # [ìˆ˜ì • í¬ì¸íŠ¸] 'Sound Engineer' -> 'Vibe Curator'ë¡œ ë³€ê²½
    # ì†Œë¦¬ë¿ë§Œ ì•„ë‹ˆë¼ ê³„ì ˆ, ì‹œê°„, ì¥ì†Œ ê°™ì€ ëª…ì‚¬ë„ ì¤‘ìš”í•˜ë‹¤ê³  ëª…ì‹œ
    system_prompt = """
You are a Content Analyst for Music Recommendation.
Your task is to identify the **Key Subject** and **Atmosphere** of the sentence.

You MUST extract keywords in this specific order of priority:
1. **The Main Subject/Setting (Nouns)**: What is the main element? (e.g., Rain, Night, Winter, Ocean, Drive, Coffee). **This is MANDATORY.**
2. **The Sound Texture (Adjectives)**: How does it sound? (e.g., Soft, Quiet, Acoustic, Jazzy).
3. **The Emotional Vibe**: (e.g., Melancholic, Cozy, Chill).

[Strict Rules]
- **If the sentence mentions a specific weather (Rain), time (Night), or place, include it as a keyword!**
- Do not just output abstract emotions.
- Return JSON only: {"keywords": ["noun1", "adj1", "adj2", ...]}
"""

    # ìœ ì € í”„ë¡¬í”„íŠ¸: ë¬¸ì¥ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ë†“ì¹˜ì§€ ë§ë¼ê³  ê°•ì¡°
    user_prompt = f"""
Extract {k} keywords from the sentence below. 
**Make sure to include the main noun (like 'Rain' or 'City') first.**

Sentence: "{merged_text}"
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    try:
        res = requests.post(
            f"{OLLAMA_URL}/api/chat",
            json={
                "model": GEMMA3_MODEL, 
                "messages": messages, 
                "stream": False, 
                "format": "json",
                "options": {"temperature": 0.3} # 0.4 ì •ë„ë¡œ ì˜¬ë ¤ì„œ ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ë¥¼ ì ì ˆíˆ ì„ë„ë¡ ìœ ë„
            },
            timeout=60
        )
        res.raise_for_status()

        raw = (
            res.json().get("message", {}).get("content")
            or res.json().get("response")
            or ""
        ).strip()

        parsed = json.loads(raw)
        kws = parsed.get("keywords", [])
        
        # ì „ì²˜ë¦¬: ì†Œë¬¸ì, ì˜ë¬¸ë§Œ, ê¸¸ì´ ì œí•œ
        clean_kws = []
        for w in kws:
            w_clean = re.sub(r"[^a-zA-Z]", "", w.lower())
            if 2 <= len(w_clean) <= 20:
                clean_kws.append(w_clean)
        
        # ì¤‘ë³µ ì œê±°
        return list(dict.fromkeys(clean_kws))[:k]

    except Exception as e:
        print(f"âš ï¸ Keyword extraction failed: {e}")
        # Fallback: ë¬¸ì¥ì˜ ì£¼ìš” ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ ì¶”ì¶œ
        # 4ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë“¤ ì¤‘, ë¬¸ì¥ ë’¤ìª½ì— ìˆëŠ” ë‹¨ì–´(ë³´í†µ í•µì‹¬ì–´) ìš°ì„  ì¶”ì¶œ
        words = re.findall(r'\b[a-zA-Z]{4,}\b', merged_text.lower())
        return list(set(words[-k:])) if words else ["chill"]
    

# =========================================================
# ì„¸ì…˜ ì €ì¥
# =========================================================
def save_to_session_simple(data, session_file):
    default_structure = {
        "session_start": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "input_korean": [],
        "input_image": [],
        "english_text_from_agent1": [],
        "english_caption_from_agent2": [],
        "merged_sentence": [],
        "english_keywords": [],
        "recommended_songs": []
    }

    if not os.path.exists(session_file):
        session_data = default_structure
    else:
        try:
            with open(session_file, "r", encoding="utf-8") as f:
                session_data = json.load(f)
        except:
            session_data = default_structure

    # append
    session_data["input_korean"].append(data["input"]["korean_text"])
    session_data["input_image"].append(data["input"]["image_path"])
    session_data["english_text_from_agent1"].append(data["english_text_from_agent1"])
    session_data["english_caption_from_agent2"].append(data["english_caption_from_agent2"])
    session_data["merged_sentence"].append(data["merged_sentence"])
    session_data["english_keywords"].append(data["english_keywords"])
    session_data["recommended_songs"].append(data["recommended_songs"])

    with open(session_file, "w", encoding="utf-8") as f:
        json.dump(session_data, f, ensure_ascii=False, indent=2)


# =========================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =========================================================
def run_agent_pipeline(korean_text="", image_path=""):
    session_file = os.path.join(SAVE_DIR, "active_session.json")
    full_history = get_full_conversation_history(session_file)

    # Agent 1
    english_text = korean_to_english(korean_text) if korean_text else ""

    # Agent 2
    english_caption = image_to_english_caption(image_path) if image_path else ""

    # Agent 3-1
    merged_sentence = rewrite_combined_sentence(english_text, english_caption, full_history)

    # Agent 3-2
    keywords = extract_keywords(merged_sentence, k=5)

    # ----------------------------------------------------
    # STEP 1: ì„¸ì…˜ì— ë¨¼ì € keywords í¬í•¨í•˜ì—¬ ì €ì¥ (=ë”± í•œ ë²ˆ ì €ì¥)
    # ----------------------------------------------------
    data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "input": {"korean_text": korean_text, "image_path": image_path},
        "english_text_from_agent1": english_text,
        "english_caption_from_agent2": english_caption,
        "merged_sentence": merged_sentence,
        "english_keywords": keywords,
        "recommended_songs": []   # ë‚˜ì¤‘ì— update
    }

    save_to_session_simple(data, session_file)

    # ----------------------------------------------------
    # STEP 2: ì„ë² ë”© ìƒì„± (ì´ë¯¸ keywords ì €ì¥ëœ ìƒíƒœ)
    # ----------------------------------------------------
    from user_keyword_embedding import (
        get_active_session_id,
        get_user_keyword_embedding_active,
        get_weighted_user_embedding_active,
        save_embedding
    )

    session_id = get_active_session_id()

    latest_emb = get_user_keyword_embedding_active()
    save_embedding(latest_emb, session_id, mode="latest")

    weighted_emb = get_weighted_user_embedding_active()
    save_embedding(weighted_emb, session_id, mode="weighted")

    # ----------------------------------------------------
    # STEP 3: ì½”ì‚¬ì¸ ì¶”ì²œ ì‹¤í–‰
    # ----------------------------------------------------
    recommended_songs_df = recommend(top_k=5)

    recommended_songs = recommended_songs_df[[
        "id", 
        "track_name",
        "artist_name",
        "recommend_score"
    ]].to_dict(orient="records")
    
    def make_embed_url(track_id):
        return f"https://open.spotify.com/embed/track/{track_id}"
    
    for song in recommended_songs:
        track_id = song["id"]

        # ì•¨ë²” ì»¤ë²„
        song["album_cover_url"] = get_album_cover_url(track_id)

        # preview_url
        song["preview_url"] = get_preview_url(track_id)
        
        song["spotify_embed_url"] = make_embed_url(track_id)

    # ----------------------------------------------------
    # STEP 4: ë°©ê¸ˆ ì €ì¥ëœ ë§ˆì§€ë§‰ ìš”ì†Œì˜ recommended_songsë§Œ ìˆ˜ì • (append í•˜ì§€ ì•ŠìŒ)
    # ----------------------------------------------------
    with open(session_file, "r", encoding="utf-8") as f:
        session_data = json.load(f)

    last_index = len(session_data["recommended_songs"]) - 1
    session_data["recommended_songs"][last_index] = recommended_songs

    with open(session_file, "w", encoding="utf-8") as f:
        json.dump(session_data, f, ensure_ascii=False, indent=2)
        
    data["recommended_songs"] = recommended_songs # ìˆ˜ì •


    return data


# =========================================================
# CLI (ì„¸ì…˜)
# =========================================================
from collections import OrderedDict

if __name__ == "__main__":
    print("\nğŸ¤– Agent Pipeline (Cosine Recommender Version)")

    active_session_path = os.path.join(SAVE_DIR, "active_session.json")
    choice = input("ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´ 'new' ì…ë ¥ (Enter: ê¸°ì¡´ ì´ì–´ì„œ ì§„í–‰): ").strip().lower()

    if choice == "new":
        if os.path.exists(active_session_path):
            with open(active_session_path, "r", encoding="utf-8") as f:
                old = json.load(f)
            old["session_end"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            archive_name = f"session_{old.get('session_id', uuid.uuid4().hex[:6])}.json"
            with open(os.path.join(SAVE_DIR, archive_name), "w", encoding="utf-8") as f:
                json.dump(old, f, ensure_ascii=False, indent=2)
            os.remove(active_session_path)

        # ìƒˆ ì„¸ì…˜ ìƒì„±
        session_id = uuid.uuid4().hex[:6]
        new_session = OrderedDict([
            ("session_id", session_id),
            ("session_start", datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
            ("input_korean", []),
            ("input_image", []),
            ("english_text_from_agent1", []),
            ("english_caption_from_agent2", []),
            ("merged_sentence", []),
            ("english_keywords", []),
            ("recommended_songs", [])
        ])
        with open(active_session_path, "w", encoding="utf-8") as f:
            json.dump(new_session, f, ensure_ascii=False, indent=2)

        print(f"ğŸ†• ìƒˆ ì„¸ì…˜ ìƒì„± ì™„ë£Œ (ID: {session_id})")
    else:
        print("â¡ ê¸°ì¡´ ì„¸ì…˜ ì´ì–´ì„œ ì§„í–‰í•©ë‹ˆë‹¤.\n")

    text = input("í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì…ë ¥: ").strip()
    img = input("ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥ (Enter í—ˆìš©): ").strip()

    result = run_agent_pipeline(text, img)
    print(json.dumps(result, indent=2, ensure_ascii=False))
