# main pipeline
# -*- coding: utf-8 -*-
"""
Agent3 (í†µí•© íŒŒì´í”„ë¼ì¸)
- Agent 1: í•œêµ­ì–´ â†’ ì˜ì–´ (EXAONE)
- Agent 2: ì´ë¯¸ì§€ â†’ ì˜ì–´ ìº¡ì…˜ (Gemma3)
- Agent 3: ë¬¸ì¥ í•©ì„± + í‚¤ì›Œë“œ ì¶”ì¶œ (Gemma3)
- ì¶”ì²œ: ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ Spotify ì˜¤ë””ì˜¤í”¼ì²˜ ì¶”ì²œ
- ì„¸ì…˜ ì €ì¥: active_session.json
"""

import os
import re
import json
from datetime import datetime
import requests
import uuid

# --------------------------
# Agent 1: ë²ˆì—­ ëª¨ë“ˆ
# --------------------------
try:
    from agent1_exaone import korean_to_english
except ImportError:
    print("âŒ 'agent1_exaone.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --------------------------
# Agent 2: ì´ë¯¸ì§€ ìº¡ì…”ë‹
# --------------------------
try:
    from agent2_imageToEng import image_to_english_caption
except ImportError:
    print("âŒ 'agent2_imageToEng.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --------------------------
# Context Manager
# --------------------------
try:
    from context_manager import get_full_conversation_history
except ImportError:
    print("âŒ 'context_manager.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# --------------------------
# Cosine Recommender
# --------------------------
try:
    from cosine_similarity_recommend import recommend, get_album_cover_url, get_preview_url
except ImportError:
    print("âŒ 'spotify/cosine_similarity_recommend.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# =========================================================
# ì „ì—­ ì„¤ì •
# =========================================================
OLLAMA_URL = "http://localhost:11434"
GEMMA3_MODEL = "gemma3:27b"
SAVE_DIR = "agents/keywords"
os.makedirs(SAVE_DIR, exist_ok=True)


# =========================================================
# Agent 3-1: ë¬¸ì¥ í•©ì„±
# =========================================================
def rewrite_combined_sentence(text1: str, text2: str, full_history: str) -> str:    
    # ì…ë ¥ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    if not text1 and not text2:
        print("âš ï¸ [Agent 3] No input text provided.")
        return ""

    print("ğŸ§© [Agent 3] Synthesizing sentences (Blending)...")

    # ê°•ë ¥í•œ ì§€ì‹œì‚¬í•­ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸
    prompt = f"""
You are a creative writer. 
Your task is to **synthesize** the following inputs into **one single, cohesive, and atmospheric English sentence**.

[Inputs]
1. User Text: "{text1}"
2. Visual Context: "{text2}"
3. Conversation History: "{full_history}"

[Rules]
- Do NOT simply concatenate the sentences.
- Blend the meaning of the text with the atmosphere of the visual.
- Make it sound natural and emotional.
- Output **ONLY** the final rewritten sentence. Do not add explanations.
"""

    messages = [{"role": "user", "content": prompt}]
    
    # Gemma3 27bëŠ” ë¬´ê±°ìš°ë¯€ë¡œ íƒ€ì„ì•„ì›ƒì„ ë„‰ë„‰íˆ ì¤Œ
    payload = {
        "model": GEMMA3_MODEL, 
        "messages": messages, 
        "stream": False, 
        "options": {
            "temperature": 0.7,  # ì°½ì˜ì ì¸ í•©ì„±ì„ ìœ„í•´ ì•½ê°„ ë†’ì„
            "num_ctx": 4096
        }
    }

    try:
        # Timeoutì„ 60ì´ˆ -> 120ì´ˆë¡œ ì¦ê°€ (27b ëª¨ë¸ ëŒ€ë¹„)
        res = requests.post(f"{OLLAMA_URL}/api/chat", json=payload, timeout=120)
        res.raise_for_status()
        
        raw = res.json().get("message", {}).get("content", "").strip()
        print(f"ğŸ” [Debug] Raw LLM Output: {raw}") # ë””ë²„ê¹…ìš© ì¶œë ¥

        # ë”°ì˜´í‘œ ì œê±° ë¡œì§ ê°œì„  (Regex ëŒ€ì‹  strip ì‚¬ìš©)
        # ëª¨ë¸ì´ ê°€ë” "Here is the sentence: ..." ë¼ê³  ë§í•  ë•Œë¥¼ ëŒ€ë¹„í•´ ì•ë’¤ë§Œ ì •ë¦¬
        cleaned_sentence = raw.strip().strip('"').strip("'")
        
        # ë§Œì•½ ëª¨ë¸ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹ˆ ê°’ì„ ë±‰ìœ¼ë©´ fallback
        if len(cleaned_sentence) < 5:
             raise ValueError("Output too short")

        return cleaned_sentence

    except Exception as e:
        print(f"âš ï¸ Merge failed: {e}")
        print("ğŸ‘‰ Using simple concatenation as fallback.")
        # ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ ê²°í•©
        return f"{text1} {text2}".strip()

# =========================================================
# Agent 3-2: ê°ì„± í‚¤ì›Œë“œ ì¶”ì¶œ
# =========================================================
def extract_keywords(merged_text: str, k: int = 3):
    if not merged_text.strip():
        return []

    print("ğŸ’¬ [Agent 3] Extracting keywords...")

    system_prompt = """
You are an expert keyword extractor.
Return JSON only:
{"keywords": ["a","b","c"]}
"""

    user_prompt = f"""
Extract {k} English keywords that represent the mood or style of this sentence:

"{merged_text}"
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    res = requests.post(
        f"{OLLAMA_URL}/api/chat",
        json={"model": GEMMA3_MODEL, "messages": messages, "stream": False, "format": "json"},
        timeout=60
    )

    raw = (
        res.json().get("message", {}).get("content")
        or res.json().get("response")
        or ""
    ).strip()

    try:
        parsed = json.loads(raw)
        kws = parsed.get("keywords", [])
        kws = [w.lower().strip() for w in kws if 2 <= len(w) <= 15]
        return kws[:k]
    except:
        # fallback: ë‹¨ì–´ë§Œ ì¶”ì¶œ
        raw_clean = re.sub(r"[^a-zA-Z ]", " ", raw)
        kws = [w for w in raw_clean.split() if len(w) > 2]
        return kws[:k]


# =========================================================
# ì„¸ì…˜ ì €ì¥
# =========================================================
def save_to_session_simple(data, session_file):
    default_structure = {
        "session_start": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "input_korean": [],
        "input_image": [],
        "english_text_from_agent1": [],
        "english_caption_from_agent2": [],
        "merged_sentence": [],
        "english_keywords": [],
        "recommended_songs": []
    }

    if not os.path.exists(session_file):
        session_data = default_structure
    else:
        try:
            with open(session_file, "r", encoding="utf-8") as f:
                session_data = json.load(f)
        except:
            session_data = default_structure

    # append
    session_data["input_korean"].append(data["input"]["korean_text"])
    session_data["input_image"].append(data["input"]["image_path"])
    session_data["english_text_from_agent1"].append(data["english_text_from_agent1"])
    session_data["english_caption_from_agent2"].append(data["english_caption_from_agent2"])
    session_data["merged_sentence"].append(data["merged_sentence"])
    session_data["english_keywords"].append(data["english_keywords"])
    session_data["recommended_songs"].append(data["recommended_songs"])

    with open(session_file, "w", encoding="utf-8") as f:
        json.dump(session_data, f, ensure_ascii=False, indent=2)


# =========================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸
# =========================================================
def run_agent_pipeline(korean_text="", image_path=""):
    session_file = os.path.join(SAVE_DIR, "active_session.json")
    full_history = get_full_conversation_history(session_file)

    # Agent 1
    english_text = korean_to_english(korean_text) if korean_text else ""

    # Agent 2
    english_caption = image_to_english_caption(image_path) if image_path else ""

    # Agent 3-1
    merged_sentence = rewrite_combined_sentence(english_text, english_caption, full_history)

    # Agent 3-2
    keywords = extract_keywords(merged_sentence, k=3)

    # ----------------------------------------------------
    # STEP 1: ì„¸ì…˜ì— ë¨¼ì € keywords í¬í•¨í•˜ì—¬ ì €ì¥ (=ë”± í•œ ë²ˆ ì €ì¥)
    # ----------------------------------------------------
    data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "input": {"korean_text": korean_text, "image_path": image_path},
        "english_text_from_agent1": english_text,
        "english_caption_from_agent2": english_caption,
        "merged_sentence": merged_sentence,
        "english_keywords": keywords,
        "recommended_songs": []   # ë‚˜ì¤‘ì— update
    }

    save_to_session_simple(data, session_file)

    # ----------------------------------------------------
    # STEP 2: ì„ë² ë”© ìƒì„± (ì´ë¯¸ keywords ì €ì¥ëœ ìƒíƒœ)
    # ----------------------------------------------------
    from user_keyword_embedding import (
        get_active_session_id,
        get_user_keyword_embedding_active,
        get_weighted_user_embedding_active,
        save_embedding
    )

    session_id = get_active_session_id()

    latest_emb = get_user_keyword_embedding_active()
    save_embedding(latest_emb, session_id, mode="latest")

    weighted_emb = get_weighted_user_embedding_active()
    save_embedding(weighted_emb, session_id, mode="weighted")

    # ----------------------------------------------------
    # STEP 3: ì½”ì‚¬ì¸ ì¶”ì²œ ì‹¤í–‰
    # ----------------------------------------------------
    recommended_songs_df = recommend(top_k=5)

    recommended_songs = recommended_songs_df[[
        "id", 
        "track_name",
        "artist_name",
        "recommend_score"
    ]].to_dict(orient="records")
    
    def make_embed_url(track_id):
        return f"https://open.spotify.com/embed/track/{track_id}"
    
    for song in recommended_songs:
        track_id = song["id"]

        # ì•¨ë²” ì»¤ë²„
        song["album_cover_url"] = get_album_cover_url(track_id)

        # preview_url
        song["preview_url"] = get_preview_url(track_id)
        
        song["spotify_embed_url"] = make_embed_url(track_id)

    # ----------------------------------------------------
    # STEP 4: ë°©ê¸ˆ ì €ì¥ëœ ë§ˆì§€ë§‰ ìš”ì†Œì˜ recommended_songsë§Œ ìˆ˜ì • (append í•˜ì§€ ì•ŠìŒ)
    # ----------------------------------------------------
    with open(session_file, "r", encoding="utf-8") as f:
        session_data = json.load(f)

    last_index = len(session_data["recommended_songs"]) - 1
    session_data["recommended_songs"][last_index] = recommended_songs

    with open(session_file, "w", encoding="utf-8") as f:
        json.dump(session_data, f, ensure_ascii=False, indent=2)
        
    data["recommended_songs"] = recommended_songs # ìˆ˜ì •


    return data


# =========================================================
# CLI (ì„¸ì…˜)
# =========================================================
from collections import OrderedDict

if __name__ == "__main__":
    print("\nğŸ¤– Agent Pipeline (Cosine Recommender Version)")

    active_session_path = os.path.join(SAVE_DIR, "active_session.json")
    choice = input("ìƒˆ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´ 'new' ì…ë ¥ (Enter: ê¸°ì¡´ ì´ì–´ì„œ ì§„í–‰): ").strip().lower()

    if choice == "new":
        if os.path.exists(active_session_path):
            with open(active_session_path, "r", encoding="utf-8") as f:
                old = json.load(f)
            old["session_end"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

            archive_name = f"session_{old.get('session_id', uuid.uuid4().hex[:6])}.json"
            with open(os.path.join(SAVE_DIR, archive_name), "w", encoding="utf-8") as f:
                json.dump(old, f, ensure_ascii=False, indent=2)
            os.remove(active_session_path)

        # ìƒˆ ì„¸ì…˜ ìƒì„±
        session_id = uuid.uuid4().hex[:6]
        new_session = OrderedDict([
            ("session_id", session_id),
            ("session_start", datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
            ("input_korean", []),
            ("input_image", []),
            ("english_text_from_agent1", []),
            ("english_caption_from_agent2", []),
            ("merged_sentence", []),
            ("english_keywords", []),
            ("recommended_songs", [])
        ])
        with open(active_session_path, "w", encoding="utf-8") as f:
            json.dump(new_session, f, ensure_ascii=False, indent=2)

        print(f"ğŸ†• ìƒˆ ì„¸ì…˜ ìƒì„± ì™„ë£Œ (ID: {session_id})")
    else:
        print("â¡ ê¸°ì¡´ ì„¸ì…˜ ì´ì–´ì„œ ì§„í–‰í•©ë‹ˆë‹¤.\n")

    text = input("í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì…ë ¥: ").strip()
    img = input("ì´ë¯¸ì§€ ê²½ë¡œ ì…ë ¥ (Enter í—ˆìš©): ").strip()

    result = run_agent_pipeline(text, img)
    print(json.dumps(result, indent=2, ensure_ascii=False))
