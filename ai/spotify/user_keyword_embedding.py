import os
import json
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModel
import numpy as np

MODEL_NAME = "michellejieli/emotion_text_classifier"
SESSION_DIR = "agents/keywords"
EMBED_SAVE_DIR = "spotify/embedding_data"
os.makedirs(EMBED_SAVE_DIR, exist_ok=True)

# -----------------------------
# 1) ëª¨ë¸ ë¡œë“œ
# -----------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"

_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
_model = AutoModel.from_pretrained(MODEL_NAME)


# -----------------------------
# 2) í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
# -----------------------------
def get_text_embedding(text: str):
    inputs = _tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)

    with torch.no_grad():
        outputs = _model(**inputs, output_hidden_states=True)

    hidden = outputs.hidden_states[-1]     # ë§ˆì§€ë§‰ ë ˆì´ì–´
    embedding = hidden.mean(dim=1).squeeze()

    # CPUë¡œ ì´ë™ í›„ numpy ë³€í™˜
    embedding = embedding.cpu().numpy().astype(np.float32)

    # ì„ë² ë”© ì •ê·œí™”
    norm = np.linalg.norm(embedding)
    if norm > 0:
        embedding = embedding / norm

    return embedding


# -----------------------------
# 3) ê°€ì¥ ìµœì‹  session_endë¥¼ ê°€ì§„ JSON ì°¾ê¸°
# -----------------------------
def get_latest_session_file_by_endtime():
    candidates = []

    for fname in os.listdir(SESSION_DIR):
        if not fname.startswith("session_") or not fname.endswith(".json"):
            continue
        
        path = os.path.join(SESSION_DIR, fname)

        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            session_end = data.get("session_end", None)
            if session_end:
                end_time = datetime.strptime(session_end, "%Y-%m-%d %H:%M:%S")
                candidates.append((end_time, path))
        except:
            continue

    if not candidates:
        raise FileNotFoundError("session_endë¥¼ í¬í•¨í•œ ì„¸ì…˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ê°€ì¥ ìµœê·¼ session_end
    latest_path = max(candidates, key=lambda x: x[0])[1]
    return latest_path


# -----------------------------
# 4) ìµœì‹  í‚¤ì›Œë“œë§Œ ì„ë² ë”© ì¶”ì¶œ
# -----------------------------
def get_user_keyword_embedding():
    session_path = get_latest_session_file_by_endtime()

    with open(session_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    keywords_all = data.get("english_keywords", [])
    if not keywords_all:
        raise ValueError("ì„¸ì…˜ íŒŒì¼ì— english_keywordsê°€ ì—†ìŠµë‹ˆë‹¤.")

    latest_keywords = keywords_all[-1]  # ê°€ì¥ ë§ˆì§€ë§‰ í‚¤ì›Œë“œ ì„¸íŠ¸
    merged_text = " ".join(latest_keywords)

    print(f"ğŸ§  ìµœì‹  ì„¸ì…˜: {session_path}")
    print(f"ğŸ“ ìµœì‹  í‚¤ì›Œë“œ: {latest_keywords}")

    return get_text_embedding(merged_text)

# -----------------------------
# 5) ì—¬ëŸ¬ í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ ì ìš© (ì¶”ì²œìš©)
# -----------------------------
def get_weighted_user_embedding():
    session_path = get_latest_session_file_by_endtime()
    with open(session_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    all_keyword_sets = data.get("english_keywords", [])
    if not all_keyword_sets:
        raise ValueError("english_keywords not found in session")

    embeddings = []
    weights = []

    # weight increases linearly for newer entries
    n = len(all_keyword_sets)

    for i, kw_list in enumerate(all_keyword_sets):
        text = " ".join(kw_list)
        emb = get_text_embedding(text)
        embeddings.append(emb)

        # ê°€ì¤‘ì¹˜: 0.5 â†’ ... â†’ 1.0
        w = 0.5 + 0.5 * (i / (n - 1)) if n > 1 else 1.0
        weights.append(w)

    embeddings = np.vstack(embeddings)
    weights = np.array(weights).reshape(-1, 1)

    weighted_vec = (embeddings * weights).sum(axis=0) / weights.sum()
    normalized = weighted_vec / np.linalg.norm(weighted_vec)
    
    return normalized.astype(np.float32)

# -----------------------------
# 6) ì„¸ì…˜ id ì¶”ì¶œ í•¨ìˆ˜
# -----------------------------
def extract_session_id(session_path: str):
    """
    filename: session_abc123.json â†’ abc123
    """
    fname = os.path.basename(session_path)
    return fname.replace("session_", "").replace(".json", "")

# -----------------------------
# 7) ì„ë² ë”© ì €ì¥ í•¨ìˆ˜
# -----------------------------
def save_embedding(embedding: np.ndarray, session_id: str, mode: str = "latest"):
    """
    embedding: numpy vector
    session_id: "abc123"
    mode: "latest" or "weighted"
    """
    fname = f"user_keyword_embedding_{session_id}_{mode}.npy"
    path = os.path.join(EMBED_SAVE_DIR, fname)
    np.save(path, embedding)
    print(f"ğŸ’¾ Saved {mode} embedding â†’ {path}")


if __name__ == "__main__":
    session_path = get_latest_session_file_by_endtime()
    session_id = extract_session_id(session_path)
    
    # ìµœì‹  í‚¤ì›Œë“œë§Œ
    latest_emb = get_user_keyword_embedding()
    print("ğŸ“Œ ìµœì‹  ì„ë² ë”© shape:", latest_emb.shape)
    save_embedding(latest_emb, session_id, mode="latest")

    # ê°€ì¤‘ í‰ê·  ì„ë² ë”©
    weighted_emb = get_weighted_user_embedding()
    print("ğŸ“Œ ê°€ì¤‘ í‰ê·  ì„ë² ë”© shape:", weighted_emb.shape)
    save_embedding(weighted_emb, session_id, mode="weighted")
